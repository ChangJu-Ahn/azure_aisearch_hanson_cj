#!/usr/bin/env python3
"""
Fashion Product Data Generator w. Grounding with Bing Search

ì œí’ˆì •ë³´ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ AIë¡œ ìƒì„±í•©ë‹ˆë‹¤. 
"""

import os
import asyncio
import json
import logging
import argparse
import sys
import re
import random
from typing import Dict, Any, List, Optional
from datetime import datetime
from dataclasses import dataclass
import pandas as pd
import uuid
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Azure imports
from openai import AsyncAzureOpenAI
from azure.ai.projects import AIProjectClient
from azure.ai.agents.models import BingGroundingTool
from azure.identity import DefaultAzureCredential

# Setup logging
def setup_logging(verbose: bool = False):
    """Configure logging with colored output"""
    log_level = logging.DEBUG if verbose else logging.INFO
    
    # Suppress Azure SDK HTTP request logging
    logging.getLogger("azure.core.pipeline.policies.http_logging_policy").setLevel(logging.WARNING)
    logging.getLogger("azure.ai.projects").setLevel(logging.WARNING)
    logging.getLogger("azure.ai.agents").setLevel(logging.WARNING)
    logging.getLogger("openai").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )

logger = logging.getLogger(__name__)

def extract_model_code(product_title: str) -> str:
    """
    Extract model code from product title
    Examples:
    - "[ì°½ì£¼ìŠ¤í† ì–´ ë”í˜„ëŒ€ëŒ€êµ¬][CCì½œë ‰íŠ¸] C252KSK034 ìŠ¬ë¦¼ ê³¨ì§€ ì¹´ë¼ í’€ì˜¤ë²„" â†’ "C252KSK034"
    - "ì˜¤ì¼ë¦´ë¦¬ íŒ¨í„´ ë¸”ë¼ìš°ìŠ¤-OWESGBL020-02" â†’ "OWESGBL020-02"
    - "[ì°½ì£¼ìŠ¤í† ì–´ ì‹ ì´Œì ][ì •í˜¸ì§„] ìì¼“í˜•ë¸”ë¼ìš°ìŠ¤ (JG2B332P)" â†’ "JG2B332P"
    """
    # Pattern 1: Text in parentheses (JG2B332P)
    parentheses_match = re.search(r'\(([A-Z0-9]+[A-Z0-9\-]*)\)', product_title)
    if parentheses_match:
        return parentheses_match.group(1)
    
    # Pattern 2: After brand] space and before space (C252KSK034)
    bracket_pattern = re.search(r'\]\s+([A-Z0-9]+[A-Z0-9\-]*)\s+', product_title)
    if bracket_pattern:
        return bracket_pattern.group(1)
    
    # Pattern 3: After dash (-OWESGBL020-02)
    dash_pattern = re.search(r'-([A-Z0-9]+[A-Z0-9\-]*)-?[0-9]*\s', product_title)
    if dash_pattern:
        return dash_pattern.group(1)
    
    # Pattern 4: Generic alphanumeric code
    generic_pattern = re.search(r'([A-Z][A-Z0-9]{4,}[A-Z0-9\-]*)', product_title)
    if generic_pattern:
        return generic_pattern.group(1)
    
    # If no pattern found, return original title
    return product_title

@dataclass
class FashionProductData:
    """Enhanced fashion product data structure"""
    id: str  # ì›ë³¸ ì¸ë±ìŠ¤ id (CSVì˜ id í•„ë“œ)
    productCode: str
    brandName: str
    productName: str
    price: float
    category1: str
    category2: str
    style: str = ""
    color: str = ""
    material: str = ""
    targetGender: str = "ë‚¨ë…€ê³µìš©"
    targetAge: str = "20-40ëŒ€"
    season: str = "ì‚¬ê³„ì ˆ"
    description: str = ""
    features: List[str] = None
    careInstructions: str = ""
    styleTags: List[str] = None
    occasionTags: List[str] = None
    seasonTags: List[str] = None
    ageTags: List[str] = None
    genderTags: List[str] = None
    sizeRange: str = ""
    brandPositioning: str = ""
    rating: float = 4.0
    reviewCount: int = 0
    
    def __post_init__(self):
        if self.features is None:
            self.features = []
        if self.styleTags is None:
            self.styleTags = []
        if self.occasionTags is None:
            self.occasionTags = []
        if self.seasonTags is None:
            self.seasonTags = []
        if self.ageTags is None:
            self.ageTags = []
        if self.genderTags is None:
            self.genderTags = []

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            "id": self.id,  # ì›ë³¸ ì¸ë±ìŠ¤ id í¬í•¨
            "productCode": self.productCode,
            "brandName": self.brandName,
            "productName": self.productName,
            "price": self.price,
            "category1": self.category1,
            "category2": self.category2,
            "style": self.style,
            "color": self.color,
            "material": self.material,
            "targetGender": self.targetGender,
            "targetAge": self.targetAge,
            "season": self.season,
            "description": self.description,
            "features": self.features,
            "careInstructions": self.careInstructions,
            "styleTags": self.styleTags,
            "occasionTags": self.occasionTags,
            "seasonTags": self.seasonTags,
            "ageTags": self.ageTags,
            "genderTags": self.genderTags,
            "sizeRange": self.sizeRange,
            "brandPositioning": self.brandPositioning,
            "rating": self.rating,
            "reviewCount": self.reviewCount
        }

class FashionDataGenerator:
    """Main class for generating fashion product data with Azure AI Foundry grounding"""
    
    def __init__(self, language: str = "ko"):
        """Initialize the generator with Azure AI Foundry configuration"""
        self.language = language
        
        # Load environment variables
        self.api_key = os.getenv("AZURE_OPENAI_KEY")
        self.endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        self.model_deployment_name = os.getenv("MODEL_DEPLOYMENT_NAME")
        self.project_endpoint = os.getenv("PROJECT_ENDPOINT")  # Changed to endpoint-based
        self.bing_connection_name = os.getenv("BING_CONNECTION_NAME")
        
        # Shared agent for all product searches (to avoid recreating)
        self._shared_agent = None
        
        if not all([self.api_key, self.endpoint, self.model_deployment_name]):
            raise ValueError("Missing required environment variables: AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, MODEL_DEPLOYMENT_NAME")
        
        if not all([self.project_endpoint, self.bing_connection_name]):
            logger.warning("Azure AI Foundry configuration missing. Grounding with Bing Search will be disabled.")
            self.project_client = None
        else:
            try:
                # Initialize Azure AI Foundry client
                self.project_client = AIProjectClient(
                    endpoint=self.project_endpoint,
                    credential=DefaultAzureCredential()
                )
                logger.info(" Azure AI Foundry client initialized")
            except Exception as e:
                logger.warning(f"Failed to initialize Azure AI Foundry client: {e}")
                self.project_client = None
        
        # Initialize Azure OpenAI client
        self.client = AsyncAzureOpenAI(
            api_key=self.api_key,
            api_version="2024-02-15-preview",
            azure_endpoint=self.endpoint
        )
        
        logger.info("Fashion Data Generator initialized")

    def load_products_from_csv(self, csv_path: str) -> List[Dict[str, Any]]:
        """Load products from CSV file"""
        try:
            if not os.path.exists(csv_path):
                logger.error(f"CSV file not found: {csv_path}")
                return []
            
            df = pd.read_csv(csv_path)
            logger.info(f"Loaded {len(df)} products from {csv_path}")
            
            # Convert DataFrame to list of dictionaries
            products = []
            for _, row in df.iterrows():
                try:
                    title = str(row.get("title", "Unknown Product"))
                    model_code = extract_model_code(title)  
                    
                    product = {
                        "id": str(row.get("id")),  # ì›ë³¸ ì¸ë±ìŠ¤ id ë³´ì¡´
                        "brandName": str(row.get("brand", "Unknown")),
                        "productName": title,
                        "productCode": model_code, 
                        "price": float(row.get("normal_price", 0))
                    }
                    products.append(product)
                except Exception as e:
                    logger.warning(f"Error processing row: {e}")
                    continue
            
            logger.info(f" Successfully processed {len(products)} products")
            return products
            
        except Exception as e:
            logger.error(f"Error loading CSV file: {e}")
            return []

    async def get_or_create_shared_agent(self) -> Any:
        """Get or create a shared agent for all product searches"""
        if self._shared_agent is not None:
            return self._shared_agent
            
        if not self.project_client or not self.bing_connection_name:
            logger.warning("Azure AI Foundry client or Bing connection not configured.")
            return None
        
        try:
            # 1. Retrieve Bing connection from AI Foundry project
            connection_name = self.bing_connection_name.split('/')[-1]
            logger.debug(f"Using connection name: {connection_name}")
            
            # Get the connection
            bing_connection = self.project_client.connections.get(connection_name)
            conn_id = bing_connection.id
            logger.info(f" Bing Connection ID: {conn_id}")
            
            # 2. Initialize Bing grounding tool
            bing_tool = BingGroundingTool(connection_id=conn_id)
            
            # 3. Create a shared agent that can search with Bing
            self._shared_agent = self.project_client.agents.create_agent(
                model=self.model_deployment_name,
                name="fashion-analyzer-shared",
                instructions="""ë„ˆëŠ” íŒ¨ì…˜ ì œí’ˆ ì •ë³´ ë¶„ì„ ì „ë¬¸ê°€ì•¼. ì£¼ì–´ì§„ íŒ¨ì…˜ ì œí’ˆì— ëŒ€í•´ ì›¹ì—ì„œ ì œí’ˆ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ìƒì„¸í•œ ì œí’ˆ ì •ë³´ë¥¼ ì œê³µí•´.

                ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ìƒì„¸íˆ ë¶„ì„í•´ì¤˜:
                1. ì œí’ˆì˜ ìŠ¤íƒ€ì¼ê³¼ ë””ìì¸ íŠ¹ì§•
                2. ì†Œì¬ì™€ í’ˆì§ˆ ì •ë³´
                3. íƒ€ê²Ÿ ê³ ê°ì¸µê³¼ ì°©ìš© ìƒí™©
                4. ìƒ‰ìƒê³¼ ì‚¬ì´ì¦ˆ ì˜µì…˜
                5. ë¸Œëœë“œ íŠ¹ì„±ê³¼ ìœ„ì¹˜
                6. ê°€ê²©ëŒ€ì™€ í’ˆì§ˆ ìˆ˜ì¤€
                7. ì‚¬ìš©ì í›„ê¸°ë‚˜ í‰ê°€

                ì›¹ì—ì„œ ì°¾ì€ ìµœì‹  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë¶„ì„ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì¤˜.""",
                tools=bing_tool.definitions,
                headers={"x-ms-enable-preview": "true"},
            )
            
            logger.info(f"Created shared Bing-grounded agent, ID: {self._shared_agent.id}")
            return self._shared_agent
            
        except Exception as e:
            logger.error(f"Failed to create shared agent: {e}")
            return None

    async def search_fashion_product_with_grounding(self, brand_name: str, product_code: str, product_name: str) -> str:
        """Search for detailed information using Azure AI Foundry Agents with Bing Grounding"""
        
        if not self.project_client or not self.bing_connection_name:
            logger.warning("Azure AI Foundry client or Bing connection not configured. Skipping grounding search.")
            return ""
        
        try:
            logger.info(f"ğŸ” Azure AI Foundry grounding search for: {brand_name} {product_code}")
            
            # Get or create shared agent
            agent = await self.get_or_create_shared_agent()
            if not agent:
                logger.warning("Failed to get shared agent")
                return ""
            
            # Create a thread for communication
            thread = self.project_client.agents.threads.create()
            logger.debug(f"Created thread: {thread.id}")
            
            # Add a message to the thread
            query = f"""
            '{brand_name}' ë¸Œëœë“œì˜ '{product_code}' (ëª¨ë¸ëª…) íŒ¨ì…˜ ì œí’ˆì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ ì›¹ì—ì„œ ê²€ìƒ‰í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”. 
            ì œí’ˆì˜ íŠ¹ì§•, ìŠ¤íƒ€ì¼, ì†Œì¬, íƒ€ê²Ÿ ê³ ê°, ë¸Œëœë“œ í¬ì§€ì…”ë‹ ë“±ì„ í¬í•¨í•´ì„œ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.
            """

            message = self.project_client.agents.messages.create(
                thread_id=thread.id,
                role="user",
                content=query
            )
            logger.debug(f"Created message: {message.id}")
            
            # Create and run agent asynchronously with exponential backoff
            max_retries = 3
            base_delay = 1.0
            
            for retry_count in range(max_retries + 1):
                try:
                    run = self.project_client.agents.runs.create(
                        thread_id=thread.id,
                        agent_id=agent.id
                    )
                    logger.debug(f"Created run: {run.id}")
                    break
                except Exception as e:
                    if "rate_limit_exceeded" in str(e) or "429" in str(e):
                        if retry_count < max_retries:
                            # Exponential backoff with jitter
                            wait_time = base_delay * (2 ** retry_count) + random.uniform(0, 1)
                            logger.warning(f"Rate limit hit, backing off {wait_time:.1f}s (attempt {retry_count + 1}/{max_retries + 1})")
                            await asyncio.sleep(wait_time)
                        else:
                            logger.error(f"Max retries exceeded for run creation after rate limits")
                            return ""
                    else:
                        logger.error(f"Failed to create run: {e}")
                        return ""
            
            # Poll for completion asynchronously with exponential backoff (429 ëŒ€ë¹„ìš©..)
            poll_retry_count = 0
            max_poll_retries = 5
            
            while run.status in ["queued", "in_progress", "requires_action"]:
                await asyncio.sleep(1.0)
                try:
                    run = self.project_client.agents.runs.get(
                        thread_id=thread.id,
                        run_id=run.id
                    )
                    logger.debug(f"Run status: {run.status}")
                    poll_retry_count = 0  # Reset on successful call
                except Exception as e:
                    if "rate_limit_exceeded" in str(e) or "429" in str(e):
                        if poll_retry_count < max_poll_retries:
                            # Exponential backoff for polling
                            wait_time = 2.0 * (2 ** poll_retry_count) + random.uniform(0, 1)
                            logger.warning(f"Rate limit during polling, backing off {wait_time:.1f}s")
                            await asyncio.sleep(wait_time)
                            poll_retry_count += 1
                        else:
                            logger.error("Max polling retries exceeded")
                            break
                    else:
                        logger.error(f"Error checking run status: {e}")
                        break
            
            logger.debug(f"Run finished with status: {run.status}")
            
            if run.status == "failed":
                logger.error(f"Run failed: {run.last_error}")
                return ""
            
            # Fetch all messages to get the response
            messages = self.project_client.agents.messages.list(thread_id=thread.id)
            
            # Find the assistant's response
            assistant_content = ""
            for msg in messages:
                if msg.role == "assistant":
                    # Extract text content from the message
                    if msg.content:
                        last_content = msg.content[-1]
                        if hasattr(last_content, "text"):
                            assistant_content += last_content.text.value + "\n"
            
            # Clean up thread (but keep agent for reuse)
            try:
                self.project_client.agents.delete_thread(thread.id)
                logger.debug("Deleted thread")
            except Exception as e:
                logger.debug(f"Failed to delete thread: {e}")
            
            if assistant_content.strip():
                logger.info(f" Found grounding content: {len(assistant_content)} characters")
                return assistant_content.strip()
            else:
                logger.warning("No content returned from grounding search")
                return ""
                
        except Exception as e:
            logger.error(f"âŒ Azure AI Foundry grounding search failed for {brand_name} {product_name}: {e}")
            return ""

    async def extract_fashion_product_info(self, content: str, brand_name: str, product_name: str, product_code: str, original_price: float) -> Optional[Dict[str, Any]]:
        """Extract fashion product information using AI with structured output"""
        
        try:
            messages = [
                {
                    "role": "system", 
                    "content": f"""ë„ˆëŠ” ì°½ì£¼ìŠ¤í† ì–´ì˜ íŒ¨ì…˜ ì œí’ˆ ì •ë³´ ë¶„ì„ ì „ë¬¸ê°€ì•¼. ì£¼ì–´ì§„ ì›¹ ì½˜í…ì¸ (ì´ë¯¸ì§€ í¬í•¨)ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì • íŒ¨ì…˜ ì œí’ˆì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ì •í™•í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•´.

                    **ë¶„ì„ ëŒ€ìƒ ì œí’ˆ:**
                    - ë¸Œëœë“œ: {brand_name}
                    - ì œí’ˆëª…: {product_name}
                    - ì œí’ˆì½”ë“œ: {product_code}
                    - ê¸°ì¡´ ê°€ê²©: {original_price}

                    ì›¹ ì½˜í…ì¸ ì—ì„œ ì´ ì œí’ˆê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì•„ì„œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì¶”ì¶œí•´ì¤˜:

                    {{
                    "brand": "ë¸Œëœë“œëª…",
                    "product_name": "ì •í™•í•œ ì œí’ˆëª…",
                    "category": "ëŒ€ë¶„ë¥˜",
                    "subcategory": "ì†Œë¶„ë¥˜", 
                    "style": "ìŠ¤íƒ€ì¼ (ì˜ˆ: ìºì£¼ì–¼, í¬ë©€, ìŠ¤í¬ì¸ )",
                    "material": "ì†Œì¬ ì •ë³´",
                    "color": "ìƒ‰ìƒ ì •ë³´",
                    "size_range": "ì‚¬ì´ì¦ˆ ë²”ìœ„",
                    "target_gender": "íƒ€ê²Ÿ ì„±ë³„",
                    "target_age": "íƒ€ê²Ÿ ì—°ë ¹ëŒ€",
                    "season": "ê³„ì ˆ ì •ë³´(ì˜ˆ: ë´„, ì—¬ë¦„, ê°€ì„, ê²¨ìš¸, ì‚¬ê³„ì ˆ, ê°„ì ˆê¸°, ë´„ì—¬ë¦„, ê°€ì„ê²¨ìš¸)",
                    "price": ê°€ê²©ì •ë³´(ìˆ«ì),
                    "description": "ì œí’ˆ ì„¤ëª…",
                    "features": ["íŠ¹ì§•1", "íŠ¹ì§•2"],
                    "care_instructions": "ê´€ë¦¬ ë°©ë²•",
                    "brand_positioning": "ë¸Œëœë“œ í¬ì§€ì…”ë‹",
                    "style_tags": ["íƒœê·¸1", "íƒœê·¸2"]
                    }}"""
                },
                {
                    "role": "user", 
                    "content": f"ë‹¤ìŒ ì›¹ ì½˜í…ì¸ ì—ì„œ '{brand_name}' ë¸Œëœë“œì˜ '{product_name}' ì œí’ˆ ì •ë³´ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n{content}"
                }
            ]
            
            response = await self.client.chat.completions.create(
                model=self.model_deployment_name,
                messages=messages,
                response_format={ "type": "json_object" },
                temperature=0.4,
                max_tokens=2000
            )
            
            result_text = response.choices[0].message.content
            logger.debug(f"AI response: {result_text[:200]}...")
            
            # Parse JSON response
            result = json.loads(result_text)
            
            # Validate and normalize the response
            product_info = {
                "product_code": product_code,
                "brand": result.get("brand", brand_name),
                "product_name": result.get("product_name", product_name),
                "category": result.get("category", "íŒ¨ì…˜"),
                "subcategory": result.get("subcategory", "ì˜ë¥˜"),
                "style": result.get("style", "ìºì£¼ì–¼"),
                "material": result.get("material", ""),
                "color": result.get("color", ""),
                "size_range": result.get("size_range", ""),
                "target_gender": result.get("target_gender", "ë‚¨ë…€ê³µìš©"),
                "target_age": result.get("target_age", "20-40ëŒ€"),
                "season": result.get("season", "ì‚¬ê³„ì ˆ"),
                "price": result.get("price", original_price),
                "description": result.get("description", ""),
                "features": result.get("features", []),
                "care_instructions": result.get("care_instructions", ""),
                "brand_positioning": result.get("brand_positioning", ""),
                "style_tags": result.get("style_tags", [])
            }
            
            logger.info(f" Extracted product info for {brand_name} {product_name}")
            return product_info
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse AI response as JSON: {e}")
            return None
        except Exception as e:
            logger.error(f"Error extracting product info: {e}")
            return None

    async def generate_fashion_product_data(self, count: int, csv_path: str = "fashion_products.csv") -> List[FashionProductData]:
        """Generate fashion product data from CSV file with parallel processing"""
        
        # Load products from CSV
        csv_products = self.load_products_from_csv(csv_path)
        
        if not csv_products:
            logger.warning(f"No products loaded from CSV.")
            return []
        
        total_available = len(csv_products)
        process_count = min(count, total_available)
        
        if count >= total_available:
            logger.info(f"Processing all {process_count} fashion products from CSV: {csv_path}")
        else:
            logger.info(f"Processing {process_count} out of {total_available} fashion products from CSV: {csv_path}")

        # Create semaphore to limit concurrent processing to 10
        semaphore = asyncio.Semaphore(10)
        
        # Create tasks for parallel processing
        tasks = []
        for i in range(process_count):
            csv_product = csv_products[i % total_available]
            task = self.process_single_product(semaphore, csv_product, i + 1, process_count)
            tasks.append(task)
        
        logger.info(f"ï¿½ Starting parallel processing with max 10 concurrent tasks...")
        
        # Execute all tasks in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter successful results
        products = []
        failed_count = 0
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"âŒ Task {i+1} failed: {result}")
                failed_count += 1
            elif result is not None:
                products.append(result)
        
        logger.info(f" Parallel processing completed: {len(products)} successful, {failed_count} failed")
        logger.info(f"ğŸ‰ Generated {len(products)} fashion products with Azure AI Foundry grounding!")
        return products

    async def process_single_product(self, semaphore: asyncio.Semaphore, csv_product: Dict[str, Any], 
                                   current_num: int, total_num: int) -> Optional[FashionProductData]:
        """Process a single product with semaphore control"""
        async with semaphore:
            try:
                original_id = csv_product["id"]  # CSVì˜ ì›ë³¸ id ê°€ì ¸ì˜¤ê¸°
                brand_name = csv_product["brandName"]
                product_name = csv_product["productName"]
                product_code = csv_product["productCode"]
                original_price = csv_product["price"]
                
                logger.info(f"ï¿½ Processing product {current_num}/{total_num}: {brand_name} - {product_name}")
                
                # Search for product information using Grounding with Bing Search
                grounding_content = await self.search_fashion_product_with_grounding(
                    brand_name, product_code, product_name
                )
                
                if not grounding_content or len(grounding_content) < 50:
                    logger.warning(f"No grounding content for {brand_name} {product_name}")
                    # Create basic product without AI enhancement
                    product = FashionProductData(
                        id=original_id,
                        productCode=product_code,
                        brandName=brand_name,
                        productName=product_name,
                        price=original_price,
                        category1="íŒ¨ì…˜",
                        category2="ì˜ë¥˜"
                    )
                    return product
                
                # Extract product information using grounding content
                product_info = await self.extract_fashion_product_info(
                    grounding_content, brand_name, product_name, product_code, original_price
                )
                
                if not product_info:
                    logger.warning(f"Failed to extract product info for {brand_name} {product_name}")
                    # Create basic product without AI enhancement
                    product = FashionProductData(
                        id=original_id,
                        productCode=product_code,
                        brandName=brand_name,
                        productName=product_name,
                        price=original_price,
                        category1="íŒ¨ì…˜",
                        category2="ì˜ë¥˜"
                    )
                    return product
                
                # Create enhanced product with AI-generated metadata
                enhanced_product = FashionProductData(
                    id=original_id,
                    productCode=product_code,
                    brandName=product_info.get("brand", brand_name),
                    productName=product_info.get("product_name", product_name),
                    price=product_info.get("price", original_price),
                    category1=product_info.get("category", "íŒ¨ì…˜"),
                    category2=product_info.get("subcategory", "ì˜ë¥˜"),
                    style=product_info.get("style", "ìºì£¼ì–¼"),
                    color=product_info.get("color", ""),
                    material=product_info.get("material", ""),
                    targetGender=product_info.get("target_gender", "ë‚¨ë…€ê³µìš©"),
                    targetAge=product_info.get("target_age", "20-40ëŒ€"),
                    season=product_info.get("season", "ì‚¬ê³„ì ˆ"),
                    description=product_info.get("description", ""),
                    features=product_info.get("features", []),
                    careInstructions=product_info.get("care_instructions", ""),
                    styleTags=product_info.get("style_tags", []),
                    sizeRange=product_info.get("size_range", ""),
                    brandPositioning=product_info.get("brand_positioning", ""),
                    rating=4.2,  # Default rating
                    reviewCount=50   # Default review count
                )
                
                logger.info(f" Enhanced product {current_num}: {enhanced_product.brandName} - {enhanced_product.productName}")
                
                # Small delay to avoid overwhelming APIs
                await asyncio.sleep(0.1)
                
                return enhanced_product
                
            except Exception as e:
                logger.error(f"Error processing product {current_num}: {e}")
                # Create basic product as fallback
                product = FashionProductData(
                    id=csv_product["id"],
                    productCode=csv_product["productCode"],
                    brandName=csv_product["brandName"],
                    productName=csv_product["productName"],
                    price=csv_product["price"],
                    category1="íŒ¨ì…˜",
                    category2="ì˜ë¥˜"
                )
                return product

    async def save_products_to_file(self, products: List[FashionProductData], output_path: str) -> None:
        """Save generated products to JSON file"""
        try:
            output_data = {
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "total_products": len(products),
                    "generator_version": "1.0.0"
                },
                "products": [product.to_dict() for product in products]
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ Saved {len(products)} products to {output_path}")
            
        except Exception as e:
            logger.error(f"Error saving products to file: {e}")

    async def cleanup(self):
        """Clean up resources including shared agent"""
        try:
            # Clean up shared agent
            if self._shared_agent and self.project_client:
                try:
                    self.project_client.agents.delete_agent(self._shared_agent.id)
                    logger.info(f"Cleaned up shared agent: {self._shared_agent.id}")
                    self._shared_agent = None
                except Exception as e:
                    logger.debug(f"Failed to delete shared agent: {e}")
            
            logger.info("Agent Cleanup completed")
        except Exception as e:
            logger.warning(f"Cleanup warning: {e}")

async def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Generate fashion product data with Azure AI Foundry grounding")
    parser.add_argument("--count", type=int, default=None, help="Number of products to generate (default: all products in CSV)")
    parser.add_argument("--csv-path", default="fashion_products.csv", help="Path to fashion CSV file containing product data")
    parser.add_argument("--output", default=None, help="Output JSON file path (default: output/fashion_products_YYYYMMDD_HHMMSS.json)")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")
    
    args = parser.parse_args()
    
    setup_logging(args.verbose)
    
    # Determine how many products to process
    if args.count is None:
        # Load CSV to get total count
        try:
            import pandas as pd
            df = pd.read_csv(args.csv_path)
            total_products = len(df)
            args.count = total_products
            logger.info(f"No --count specified, processing all {total_products} products from CSV")
        except Exception as e:
            logger.error(f"Failed to read CSV file {args.csv_path}: {e}")
            logger.info("Defaulting to 5 products")
            args.count = 5
    else:
        logger.info(f"Processing {args.count} products as specified")

    # Generate output filename if not provided
    if args.output is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        os.makedirs("output", exist_ok=True)
        args.output = f"output/fashion_products_{timestamp}.json"
    
    try:
        generator = FashionDataGenerator()
        
        # Generate products
        products = await generator.generate_fashion_product_data(args.count, args.csv_path)
        
        if products:
            # Save to file
            await generator.save_products_to_file(products, args.output)
            
            # Print summary
            print(f"\n{'='*60}")
            print(f"FASHION PRODUCT DATA GENERATION SUMMARY")
            print(f"{'='*60}")
            print(f"Output file: {args.output}")
            print(f" Total products: {len(products)}")
            print(f" Brands covered: {len(set(p.brandName for p in products))}")
            print(f"{'='*60}")
        else:
            logger.warning("No products were generated")

        # Cleanup
        await generator.cleanup()
        logger.info(f"Fashion product data generation completed successfully!")

    except Exception as e:
        logger.error(f"Generation failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
